{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "\n",
    "model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_tensor_video_clip(clip):\n",
    "    if not torch.is_tensor(clip):\n",
    "        raise TypeError(\"clip should be Tensor. Got %s\" % type(clip))\n",
    "\n",
    "    if not clip.ndimension() == 4:\n",
    "        raise ValueError(\"clip should be 4D. Got %dD\" % clip.dim())\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def center_crop_arr(pil_image, image_size):\n",
    "    \"\"\"\n",
    "    Center cropping implementation from ADM.\n",
    "    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n",
    "    \"\"\"\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        pil_image = pil_image.resize(\n",
    "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
    "        )\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    pil_image = pil_image.resize(\n",
    "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n",
    "\n",
    "\n",
    "def crop(clip, i, j, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    if len(clip.size()) != 4:\n",
    "        raise ValueError(\"clip should be a 4D tensor\")\n",
    "    return clip[..., i: i + h, j: j + w]\n",
    "\n",
    "\n",
    "def resize(clip, target_size, interpolation_mode):\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "    return torch.nn.functional.interpolate(clip, size=target_size, mode=interpolation_mode, align_corners=True, antialias=True)\n",
    "\n",
    "\n",
    "def resize_scale(clip, target_size, interpolation_mode):\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "    H, W = clip.size(-2), clip.size(-1)\n",
    "    scale_ = target_size[0] / min(H, W)\n",
    "    return torch.nn.functional.interpolate(clip, scale_factor=scale_, mode=interpolation_mode, align_corners=True, antialias=True)\n",
    "\n",
    "\n",
    "def resized_crop(clip, i, j, h, w, size, interpolation_mode=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Do spatial cropping and resizing to the video clip\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "        i (int): i in (i,j) i.e coordinates of the upper left corner.\n",
    "        j (int): j in (i,j) i.e coordinates of the upper left corner.\n",
    "        h (int): Height of the cropped region.\n",
    "        w (int): Width of the cropped region.\n",
    "        size (tuple(int, int)): height and width of resized clip\n",
    "    Returns:\n",
    "        clip (torch.tensor): Resized and cropped clip. Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    clip = crop(clip, i, j, h, w)\n",
    "    clip = resize(clip, size, interpolation_mode)\n",
    "    return clip\n",
    "\n",
    "\n",
    "def center_crop(clip, crop_size):\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "    th, tw = crop_size\n",
    "    if h < th or w < tw:\n",
    "        raise ValueError(\"height and width must be no smaller than crop_size\")\n",
    "\n",
    "    i = int(round((h - th) / 2.0))\n",
    "    j = int(round((w - tw) / 2.0))\n",
    "    return crop(clip, i, j, th, tw)\n",
    "\n",
    "\n",
    "def center_crop_using_short_edge(clip):\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "    if h < w:\n",
    "        th, tw = h, h\n",
    "        i = 0\n",
    "        j = int(round((w - tw) / 2.0))\n",
    "    else:\n",
    "        th, tw = w, w\n",
    "        i = int(round((h - th) / 2.0))\n",
    "        j = 0\n",
    "    return crop(clip, i, j, th, tw)\n",
    "\n",
    "class CenterCropResizeVideo:\n",
    "    '''\n",
    "    First use the short side for cropping length,\n",
    "    center crop video, then resize to the specified size\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            size,\n",
    "            interpolation_mode=\"bilinear\",\n",
    "    ):\n",
    "        if isinstance(size, tuple):\n",
    "            if len(size) != 2:\n",
    "                raise ValueError(f\"size should be tuple (height, width), instead got {size}\")\n",
    "            self.size = size\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "        Returns:\n",
    "            torch.tensor: scale resized / center cropped video clip.\n",
    "                size is (T, C, crop_size, crop_size)\n",
    "        \"\"\"\n",
    "        clip_center_crop = center_crop_using_short_edge(clip)\n",
    "        clip_center_crop_resize = resize(clip_center_crop, target_size=self.size,\n",
    "                                         interpolation_mode=self.interpolation_mode)\n",
    "        return clip_center_crop_resize\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(size={self.size}, interpolation_mode={self.interpolation_mode}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([301, 3, 128, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decord import VideoReader, cpu\n",
    "from decord import bridge\n",
    "\n",
    "bridge = bridge.set_bridge(\"torch\")\n",
    "\n",
    "video_path = \"/cvdata1/jihwan/minecraft/train/1_2/000002.mp4\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0))\n",
    "video = vr.get_batch(range(len(vr)))\n",
    "video = video.permute(0, 3, 1, 2)\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([301, 3, 128, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_crop_resize = CenterCropResizeVideo(size=128)\n",
    "resized_video = center_crop_resize(video)\n",
    "resized_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1, len(resized_video)):\n",
    "        image = resized_video[i]\n",
    "        image = Image.fromarray(image.permute(1, 2, 0).numpy())\n",
    "        image = preprocess(image)\n",
    "        image = image.unsqueeze(0).to('cuda')\n",
    "        embedding = model.encode_image(image)\n",
    "\n",
    "        embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consecutive frames\n",
      "tensor([0.9770], device='cuda:0')\n",
      "tensor([0.9710], device='cuda:0')\n",
      "tensor([0.9745], device='cuda:0')\n",
      "tensor([0.9694], device='cuda:0')\n",
      "tensor([0.9786], device='cuda:0')\n",
      "tensor([0.9703], device='cuda:0')\n",
      "tensor([0.9718], device='cuda:0')\n",
      "tensor([0.9777], device='cuda:0')\n",
      "tensor([0.9764], device='cuda:0')\n",
      "tensor([0.9744], device='cuda:0')\n",
      "tensor([0.9591], device='cuda:0')\n",
      "tensor([0.9668], device='cuda:0')\n",
      "tensor([0.9674], device='cuda:0')\n",
      "tensor([0.9424], device='cuda:0')\n",
      "tensor([0.9465], device='cuda:0')\n",
      "tensor([0.9654], device='cuda:0')\n",
      "tensor([0.9567], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "print(\"consecutive frames\")\n",
    "for i in range(1, 18):\n",
    "    print(cosine_sim(embeddings[0], embeddings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consecutive chunks\n",
      "tensor([0.9567], device='cuda:0')\n",
      "tensor([0.9423], device='cuda:0')\n",
      "tensor([0.9488], device='cuda:0')\n",
      "tensor([0.9000], device='cuda:0')\n",
      "tensor([0.9494], device='cuda:0')\n",
      "tensor([0.9488], device='cuda:0')\n",
      "tensor([0.9444], device='cuda:0')\n",
      "tensor([0.9340], device='cuda:0')\n",
      "tensor([0.9664], device='cuda:0')\n",
      "tensor([0.9467], device='cuda:0')\n",
      "tensor([0.9577], device='cuda:0')\n",
      "tensor([0.9580], device='cuda:0')\n",
      "tensor([0.9445], device='cuda:0')\n",
      "tensor([0.9540], device='cuda:0')\n",
      "tensor([0.9385], device='cuda:0')\n",
      "tensor([0.9538], device='cuda:0')\n",
      "tensor([0.9490], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"consecutive chunks\")\n",
    "for i in range(17, len(embeddings), 17):\n",
    "    print(cosine_sim(embeddings[0], embeddings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison btw the first frames of different videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8432], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_crop_resize = CenterCropResizeVideo(size=128)\n",
    "video_path_1 = \"/cvdata1/jihwan/minecraft/train/1_2/000001.mp4\"\n",
    "video_path_2 = \"/cvdata1/jihwan/minecraft/train/1_2/000002.mp4\"\n",
    "\n",
    "vr1 = VideoReader(video_path_1, ctx=cpu(0))\n",
    "video1 = vr1.get_batch(range(len(vr1)))\n",
    "video1 = video1.permute(0, 3, 1, 2)\n",
    "resized_video1 = center_crop_resize(video1)\n",
    "\n",
    "\n",
    "vr2 = VideoReader(video_path_2, ctx=cpu(0))\n",
    "video2 = vr2.get_batch(range(len(vr2)))\n",
    "video2 = video2.permute(0, 3, 1, 2)\n",
    "resized_video2 = center_crop_resize(video2)\n",
    "\n",
    "\n",
    "image1 = resized_video1[i]\n",
    "image1 = Image.fromarray(image1.permute(1, 2, 0).numpy())\n",
    "image1 = preprocess(image1)\n",
    "image1 = image1.unsqueeze(0).to('cuda')\n",
    "embedding1 = model.encode_image(image1)\n",
    "\n",
    "image2 = resized_video2[i]\n",
    "image2 = Image.fromarray(image2.permute(1, 2, 0).numpy())\n",
    "image2 = preprocess(image2)\n",
    "image2 = image2.unsqueeze(0).to('cuda')\n",
    "embedding2 = model.encode_image(image2)\n",
    "\n",
    "cosine_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
