{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/131_data/jihwan/2024_lgm/open_clip/.clip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14-378-quickgelu', pretrained='dfn5b')\n",
    "\n",
    "model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _is_tensor_video_clip(clip):\n",
    "    if not torch.is_tensor(clip):\n",
    "        raise TypeError(\"clip should be Tensor. Got %s\" % type(clip))\n",
    "\n",
    "    if not clip.ndimension() == 4:\n",
    "        raise ValueError(\"clip should be 4D. Got %dD\" % clip.dim())\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def center_crop_arr(pil_image, image_size):\n",
    "    \"\"\"\n",
    "    Center cropping implementation from ADM.\n",
    "    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n",
    "    \"\"\"\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        pil_image = pil_image.resize(\n",
    "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
    "        )\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    pil_image = pil_image.resize(\n",
    "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n",
    "\n",
    "\n",
    "def crop(clip, i, j, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    if len(clip.size()) != 4:\n",
    "        raise ValueError(\"clip should be a 4D tensor\")\n",
    "    return clip[..., i: i + h, j: j + w]\n",
    "\n",
    "\n",
    "def resize(clip, target_size, interpolation_mode):\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "    return torch.nn.functional.interpolate(clip, size=target_size, mode=interpolation_mode, align_corners=True, antialias=True)\n",
    "\n",
    "\n",
    "def resize_scale(clip, target_size, interpolation_mode):\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "    H, W = clip.size(-2), clip.size(-1)\n",
    "    scale_ = target_size[0] / min(H, W)\n",
    "    return torch.nn.functional.interpolate(clip, scale_factor=scale_, mode=interpolation_mode, align_corners=True, antialias=True)\n",
    "\n",
    "\n",
    "def resized_crop(clip, i, j, h, w, size, interpolation_mode=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Do spatial cropping and resizing to the video clip\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "        i (int): i in (i,j) i.e coordinates of the upper left corner.\n",
    "        j (int): j in (i,j) i.e coordinates of the upper left corner.\n",
    "        h (int): Height of the cropped region.\n",
    "        w (int): Width of the cropped region.\n",
    "        size (tuple(int, int)): height and width of resized clip\n",
    "    Returns:\n",
    "        clip (torch.tensor): Resized and cropped clip. Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    clip = crop(clip, i, j, h, w)\n",
    "    clip = resize(clip, size, interpolation_mode)\n",
    "    return clip\n",
    "\n",
    "\n",
    "def center_crop(clip, crop_size):\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "    th, tw = crop_size\n",
    "    if h < th or w < tw:\n",
    "        raise ValueError(\"height and width must be no smaller than crop_size\")\n",
    "\n",
    "    i = int(round((h - th) / 2.0))\n",
    "    j = int(round((w - tw) / 2.0))\n",
    "    return crop(clip, i, j, th, tw)\n",
    "\n",
    "\n",
    "def center_crop_using_short_edge(clip):\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "    if h < w:\n",
    "        th, tw = h, h\n",
    "        i = 0\n",
    "        j = int(round((w - tw) / 2.0))\n",
    "    else:\n",
    "        th, tw = w, w\n",
    "        i = int(round((h - th) / 2.0))\n",
    "        j = 0\n",
    "    return crop(clip, i, j, th, tw)\n",
    "\n",
    "class CenterCropResizeVideo:\n",
    "    '''\n",
    "    First use the short side for cropping length,\n",
    "    center crop video, then resize to the specified size\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            size,\n",
    "            interpolation_mode=\"bilinear\",\n",
    "    ):\n",
    "        if isinstance(size, tuple):\n",
    "            if len(size) != 2:\n",
    "                raise ValueError(f\"size should be tuple (height, width), instead got {size}\")\n",
    "            self.size = size\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "        Returns:\n",
    "            torch.tensor: scale resized / center cropped video clip.\n",
    "                size is (T, C, crop_size, crop_size)\n",
    "        \"\"\"\n",
    "        clip_center_crop = center_crop_using_short_edge(clip)\n",
    "        clip_center_crop_resize = resize(clip_center_crop, target_size=self.size,\n",
    "                                         interpolation_mode=self.interpolation_mode)\n",
    "        return clip_center_crop_resize\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(size={self.size}, interpolation_mode={self.interpolation_mode}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([266, 3, 1080, 1920])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decord import VideoReader, cpu\n",
    "from decord import bridge\n",
    "\n",
    "bridge = bridge.set_bridge(\"torch\")\n",
    "\n",
    "video_path = \"/cvdata1/jihwan/mixkit/Cats/mixkit-a-white-cat-sits-in-front-of-a-white-wall-1535.mp4\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0))\n",
    "video = vr.get_batch(range(len(vr)))\n",
    "video = video.permute(0, 3, 1, 2)\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([266, 3, 512, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_crop_resize = CenterCropResizeVideo(size=512)\n",
    "resized_video = center_crop_resize(video)\n",
    "resized_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1, len(resized_video)):\n",
    "        image = resized_video[i]\n",
    "        image = Image.fromarray(image.permute(1, 2, 0).numpy())\n",
    "        image = preprocess(image)\n",
    "        image = image.unsqueeze(0).to('cuda')\n",
    "        embedding = model.encode_image(image)\n",
    "\n",
    "        embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consecutive frames\n",
      "tensor([1.0000], device='cuda:0')\n",
      "tensor([0.9961], device='cuda:0')\n",
      "tensor([0.9968], device='cuda:0')\n",
      "tensor([0.9944], device='cuda:0')\n",
      "tensor([0.9931], device='cuda:0')\n",
      "tensor([0.9878], device='cuda:0')\n",
      "tensor([0.9916], device='cuda:0')\n",
      "tensor([0.9914], device='cuda:0')\n",
      "tensor([0.9919], device='cuda:0')\n",
      "tensor([0.9923], device='cuda:0')\n",
      "tensor([0.9913], device='cuda:0')\n",
      "tensor([0.9919], device='cuda:0')\n",
      "tensor([0.9919], device='cuda:0')\n",
      "tensor([0.9899], device='cuda:0')\n",
      "tensor([0.9904], device='cuda:0')\n",
      "tensor([0.9895], device='cuda:0')\n",
      "tensor([0.9920], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "print(\"consecutive frames\")\n",
    "for i in range(1, 18):\n",
    "    print(cosine_sim(embeddings[0], embeddings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consecutive chunks\n",
      "tensor([0.9920], device='cuda:0')\n",
      "tensor([0.9717], device='cuda:0')\n",
      "tensor([0.9576], device='cuda:0')\n",
      "tensor([0.9438], device='cuda:0')\n",
      "tensor([0.9116], device='cuda:0')\n",
      "tensor([0.9328], device='cuda:0')\n",
      "tensor([0.9587], device='cuda:0')\n",
      "tensor([0.9631], device='cuda:0')\n",
      "tensor([0.9648], device='cuda:0')\n",
      "tensor([0.9556], device='cuda:0')\n",
      "tensor([0.8523], device='cuda:0')\n",
      "tensor([0.8327], device='cuda:0')\n",
      "tensor([0.8380], device='cuda:0')\n",
      "tensor([0.8539], device='cuda:0')\n",
      "tensor([0.8566], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"consecutive chunks\")\n",
    "for i in range(17, len(embeddings), 17):\n",
    "    print(cosine_sim(embeddings[0], embeddings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open_clip.model.CLIP"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
